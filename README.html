<!DOCTYPE html>
<!-- saved from url=(0046)https://eec193.github.io/labs/lab3/README.html -->
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>README</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="phase-1">Phase 1</h1>
<p>In phase 1, you will be using the LISA dataset in order to make a vehicle localizer using Pytorch. Specifically, you will take the Resnet-18 pretrained model, and modify it so that the final output layer is a bounding box regressor.</p>
<h2 id="phase-1-questions">Phase 1 Questions</h2>
<p><em>Q1. How does your choice of loss affect the accuracy of your bounding box regressor. Specifically, what is the accuracy difference between L1 or L2 loss? Which works better and why? (To properly answer this question you will need to understand what is happening both conceptually as well have done some experimentation on your model).</em></p>
<p><em>Q2. What is the purpose of IoU? Why is it so important in object detection?</em></p>
<p><em>Q3. The current model in this phase can only do single object detection. How would you transform this model to handle multiple object detection? (There are multiple valid answers to this. You should compare the merits of each method.)</em></p>
<h1 id="phase-2">Phase 2</h1>
<p>In phase 2, you will be comparing how YOLO-based unified regression models do against Region Proposal Based networks on the COCO dataset. Specifically, you will need to train models using both YOLOv3 and Mask-RCNN.</p>
<h2 id="motivation">Motivation</h2>
<p>The goal of this phase is to familiarize how to benchmark the performance of state of the art deep learning algorithms on common datasets. In industry it is very common to productionize the best open source models rather than spending time developing a custom architecture as the development times are very expensive and its possible to make a very accurate model if you have enough data.</p>
<h2 id="assignment-details">Assignment Details</h2>
<h3 id="model-creation">Model Creation</h3>
<p>In this assignment you will need to produce two object detection models. For benchmark purposes you will need to have two models trained on the COCO dataset. The algorithms you will use for the object detection model are YOLOv3 and Mask-RCNN. Both algorithms have multiple open source implementations. It is up to you to choose the open source implementation for both algorithms you would like to use. There are docker images on both Kronos and Atlas that will allow you to run all of the implementations linked in the README. You will need to figure out how to train the COCO dataset. However, you do not need to go through the entire training process for both these models. Since we are so restricted in terms of compute resources, it would be impossible to finish this lab in a week because for each student it could take anywhere from 2-3 days to train both models on the entire COCO dataset. Instead, you can use pre-trained weights on the COCO dataset, as you will need to use one of the two models to produce a video for phase 3. One thing to note, is that the COCO dataset has more than just vehicles, but other various objects. However, your detector should suppress drawing bounding boxes for any non-vehicle objects.</p>
<h3 id="code-documentation">Code Documentation</h3>
<p>Your report should include detailed instructions on how to run and train both models. This includes the proper directory structure, command line inputs, python scripts, docker setup, etc. This is to help you get into the habit of creating good documentation to replicate your results. Your documentation should also show how to use your model weights to find all vehicles (not just cars, but trucks, motorcycles, etc.) in a random image.</p>
<h3 id="report">Report</h3>
<p>Your report should argue for the use of one of these models to be used as a vehicle detector. Your report will be graded on the quality of your argument for one model vs the other. Your report should discuss the accuracy of the model, the speed at which each model runs on a single GPU, and any other metrics which you consider important to a vehicle detection model. Your answer should be justified with statistics from your experimentation with each algorithm as well as from your understandings of how each model works. This can be done by citing knowledge from the original papers themselves, statistics, as well as knowledge, you have gained from class. Your report should provide a very clear argument as to why one model is better than the other, and you must be very explicit as to the model you believe works better for vehicle detection.</p>
<h2 id="links">Links</h2>
<h3 id="mask-rcnn-implementations">Mask-RCNN Implementations</h3>
<p><a href="https://github.com/multimodallearning/pytorch-mask-rcnn">Pytorch</a></p>
<p>Docker Image: pytorch/pytorch</p>
<p><a href="https://github.com/matterport/Mask_RCNN">Tensorflow</a></p>
<p>Docker Image: tensorflow/tensorflow</p>
<h3 id="yolov3-implementation">YOLOv3 Implementation</h3>
<p>YOLOv3 Code: <a href="https://github.com/ultralytics/yolov3">Pytorch</a></p>
<p>Docker Image: pytorch/pytorch</p>
<p><a href="https://pjreddie.com/darknet/yolo/">Darknet Original Implementation</a></p>
<p>Docker Image: loretoparisi/darknet</p>
<p><a href="http://cocodataset.org/#home">COCO Dataset</a></p>
<h1 id="phase-3">Phase 3</h1>
<p>After determining which model works better for vehicle detection,you must use that model to detect all vehicles in the Udacity highway video from the lane line detection lab. Please include the final video with both the detected vehicles and the detected lane lines. Please keep in mind that your final video should only include vehicles on the road. If the video includes detections of other objects it will be considered incorrect.</p>
<p>Bonus: Output the real world coordinate locations for all the vehicles detected (you just need to provide the X,Y coordinates).</p>
<h1 id="final-submission-details">Final Submission Details</h1>
<p>Your final submission must include the following things: 1. Your filled out Python notebook for Phase 1.</p>
<ol start="2" type="1">
<li><p>Full instructions in a markdown file for training/inference of both Mask-rcnn and YOLOv3 as specified in the Phase 2 instructions.</p></li>
<li><p>A report with the answers for the Phase 1 questions, and your report for Phase 2.</p></li>
<li><p>A video with all detected objects as well as lane line detections in the Udacity lane line detection video.</p></li>
</ol>


</body></html>